{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GSAPP PLA6113 Wednesday February 3rd, 2021\n",
    "# Exploring Urban Data with ML\n",
    "# Supervised Learning 2\n",
    "## Linear classifiers (probability models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contents\n",
    "\n",
    "* Supervised Learning and Classification\n",
    "* Linear Classifiers (Probability Models)\n",
    "    * Naive Bayes Classifier\n",
    "    * Logistic Regression     \n",
    "* Urban Example: Applications of machine learning methods to predict readmission and length-of-stay for homeless families: the case of WIN shelters in New York City"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Supervised Learning and Classification\n",
    "\n",
    "* Classification: subcategory of supervised learning \n",
    "* Input-output pairs\n",
    "* Goal of classification: predict the categorical class labels of new instances based on past observations\n",
    "<br>\n",
    "\n",
    "* Example: Predicting whether a neighborhood will be gentrified or not\n",
    "    * Label ($y$): binary representation of neighborhood gentrification in the past\n",
    "    * Predictors ($X$): neighborhood characteristics (demogrpahics, socioeconomics, or urban form features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Types of classification problems\n",
    "\n",
    "#### Binary classification\n",
    "The typical example is e-mail spam detection, which each e-mail is spam → 1 ; or isn’t → 0.\n",
    "#### Multi-class classification\n",
    "Like handwritten character recognition (where classes go from 0 to 9).\n",
    "\n",
    "![title](image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note of binary classification: There are 2 classes, red and blue, and 2 features, $x_1$ and $x_2$. The model is able to find the relationship between the features of each data point and its class, and to set a boundary line between them, so when provided with new data, it can estimate the class where it belongs, given its features. In this case, the new data point falls into the blue subspace and, therefore, the model will predict its class to be a blue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Types of classification algorithms (solutions)\n",
    "* Not every classification models will be useful to separate properly different classes from a dataset\n",
    "* The task of selecting an appropiate algorithm became of paramount importance in classification problems\n",
    "\n",
    "#### Linear classification\n",
    "Classes can be separated by a linear decision boundary\n",
    "\n",
    "Example algorithms: Naive Bayes Classifier, Logistic Regression (we will learn these today)\n",
    "#### Non-linear classification\n",
    "Classes can't be separated by a linear decision boundary\n",
    "\n",
    "Example algorithms: K-nearest neighbors, Kernel Support Vector Machine, Perceptron\n",
    "\n",
    "![title](image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Classifiers (Probability Models)\n",
    "\n",
    "#### Goal of probabilistic classifier \n",
    "* To determine the probability of the features occurring in each class with features $x_0$ through $x_n$ and classes $c_0$ through $c_k$\n",
    "* To return the most likely class\n",
    "* For each class, we want to calculate $P(c_i | x_0, …, x_n)$ \n",
    "* Example: <br>\n",
    "Probability that an email containing 'win' and 'lottery' is spam = $P(C_{spam} | x_{win}, x_{lottery})$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-1. Naive Bayes Classifier\n",
    "\n",
    "* A simple, yet effective and commonly-used, machine learning classifier \n",
    "* A probabilistic classifier based on the __Bayesian rule__\n",
    "* Traditional solution for problems such as spam detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Bayesian rule (Bayes' theorem)\n",
    "\n",
    "* The probability of an event, based on prior knowledge of conditions that might be related to the event\n",
    "* The probability of an event can be predicted based on the previous observations\n",
    "* How to predict a probability of class ($c$, target) given predictor ($x$, feature)\n",
    "\\begin{align}\n",
    "\\ P(c|x) = \\frac{P(x|c)\\cdot P(c)}{P(x)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Meaning, __the probability of class $c$ given predictor $x$__ (posterior proability) can be estimated based on (1) $P(x|c)$ (likelihood), (2) $P(c)$ (class prior probability), and (3) $P(x)$ (predictor prior probability) because <br>\n",
    "\n",
    "The probability of a data point is associated with class $c$ and predictor $x$ is expressed as <br>\n",
    "\n",
    "\\begin{align}\n",
    "\\ P(c\\&x) = P(c) \\cdot P(x|c) = P(x) \\cdot P(c|x)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with the spam classification example\n",
    "\n",
    "* Consider some data about content of several e-mails classified as Spam and non-spam\n",
    "\n",
    "|  Cruise | Lottery | Win | Spam |\n",
    "|-----|-------------|---------|-------|\n",
    "|  1 | 1 | 1 | 1|\n",
    "| 1 | 0 | 1 | 1 |\n",
    "| 0 | 1 | 0 | 1 |\n",
    "| 0 | 0 | 1 | 0 |\n",
    "| 1 | 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 | 0 |\n",
    "\n",
    "* Now if we receive an email containing \"Cruise\", is this spam or not?\n",
    "\\begin{align}\n",
    "\\ P(c_{spam}|x_{cruise}) = \\frac{2}{3} = \\frac{P(x_{cruise}|c_{spam})\\cdot P(c_{spam})}{P(x_{cruise})} = \\frac{\\frac{2}{3} \\cdot \\frac{1}{2}}{\\frac{1}{2}} = \\frac{2}{3}\\\\\n",
    "\\end{align}\n",
    "\n",
    "* Now if we receive an email containing \"Cruise\" and \"Lottery\", is this spam or not?\n",
    "\\begin{align}\n",
    "\\ P(c_{spam}|x_{cruise}x_{lottery}) = P(x_{cruise}|c_{spam})\\cdot P(x_{lottery}|c_{spam}) \\cdot P(!x_{win}|c_{spam}) \\cdot P(c_{spam}) = \\frac{2}{3} \\cdot \\frac{2}{3} \\cdot (1-\\frac{2}{3}) \\cdot \\frac{1}{2} = \\frac{2}{27}\\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\ P(c_{nonspam}|x_{cruise}x_{lottery}) = P(x_{cruise}|c_{nonspam})\\cdot P(x_{lottery}|c_{nonspam}) \\cdot P(!x_{win}|c_{nonspam}) \\cdot P(c_{nonspam}) = \\frac{1}{3} \\cdot \\frac{1}{3} \\cdot (1-\\frac{1}{3}) \\cdot \\frac{1}{2} = \\frac{1}{27}\\\\\n",
    "\\end{align}\n",
    "\n",
    "* Conclusion: the probability of spam is 2/27, higher than the probability of non-spam (1/27), so the email with \"Cruise\" and \"Lottery\" is classified as spam.\n",
    "\n",
    "\\begin{align}\n",
    "\\ P(c_{spam}|x_{cruise}x_{lottery}) > \\ P(c_{nonspam}|x_{cruise}x_{lottery})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Definition of Naive Bayes Classifier\n",
    "Predict a probability that a new data point with predictors $x_0, x_1,…x_n$ would be class $i$ \n",
    "\n",
    "\\begin{align}\n",
    "\\ P(c_i|x_0,x_1,…,x_n) = \\frac{P(x_0,x_1,...x_n|c_i)\\cdot P(c_i)}{P(x_0,x_1,…,x_n)} =  P(c_i) \\prod_{j=1}^{n}P(x_j|c_i)\\\\\n",
    "\\end{align}\n",
    "\n",
    "__Important assumption: Predictors $x_0$ through $x_n$ are conditionally independent given $c_i$__\n",
    "\n",
    "* This assumption allows that $P(x_0,x_1 …, x_n | c_i)$ = $P(x_0 | c_i) \\cdot P(x_1 | c_i) …  P(x_n | c_i)$. \n",
    "* This assumption is most likely not true in real-world — hence the name naive Bayes classifier, but the classifier nonetheless performs well in most situations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) What are the Pros and Cons of Naive Bayes?\n",
    "\n",
    "#### Pros\n",
    "* Simple, quick, and accurate. It is easy and fast to predict class of test data set. It also perform well in multi class prediction (less computation cost). The reason that naive Bayes models are so efficient is that they learn parameters by looking at each feature individually and collect simple per-class statistics from each feature.\n",
    "* When assumption of independence holds, it performs better compare to other models like logistic regression.\n",
    "* It performs well even though you have relatively small training data.\n",
    "* It performs well in case of categorical (discrete) input variables compared to numerical variable(s).\n",
    "\n",
    "#### Cons\n",
    "* The assumption of independent features. In practice, it is almost impossible that model will get a set of predictors which are entirely independent.\n",
    "* If there is no training tuple of a particular class, this causes zero posterior probability. In this case, the model is unable to make predictions. This problem is known as Zero Probability/Frequency Problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Types of Naive Bayes Classifiers\n",
    "Depending on what distribution your features follow\n",
    "(https://scikit-learn.org/stable/modules/naive_bayes.html#bernoulli-naive-bayes)\n",
    "\n",
    "__Bernoulli Naive Bayes Classifier__\n",
    "* The binomial model is useful if your feature vectors are binary (i.e. zeros and ones) \n",
    "* Example: a ‘bag of words’ model for a spam classification purpose where the 1s & 0s are “word occurs in the document” and “word does not occur in the document” respectively\n",
    "\n",
    "\n",
    "__Multinomial Naive Bayes Classifier__\n",
    "* It is used for discrete counts\n",
    "* Example: a document classification based on “count how often word occurs in the document”\n",
    "\n",
    "\n",
    "__Gaussian Naive Bayes Classifier__ _*** you will mostly use this algorithm with urban data_\n",
    "* It assumes that continuous features follow a normal distribution\n",
    "* Gaussian Probability Density Function (PDF) is used based on the normal distribution curve\n",
    "* Example: a at-risk building classification based on continuous predictors such as built year, building size, boiler capacity, etc.\n",
    "    \n",
    "<div>\n",
    "<img src=\"image3.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python example: Predicting a building type by using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Gaussian Naive Bayes module (GaussianNB) from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data randomly into the training (75%) and test (25%) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Gaussian Naive Bayes Classifier\n",
    "    YOUR_MODEL_NAME = GaussianNB().fit(X_train, y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy of training model\n",
    "    YOUR_MODEL_NAME.score(X_Train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High prediction accuracy. Let's apply to the test data\n",
    "You are now using your model trained previously based on your train data to predict class of your new data points (test data).\n",
    "\n",
    "    Probability of each class = YOUR_MODEL_NAME.predict_proba(X_test)\n",
    "    Classification prediction = YOUR_MODEL_NAME.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now you can predict a building type of a completely new data point\n",
    "* 2,600 sqft - gross floor area\n",
    "* 1,300 sqft - land area\n",
    "* 2015 - built year\n",
    "* $850,000 - property price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-2. Logistic Regression\n",
    "* Linear classifier (a bianry representation)\n",
    "* Works reasonably well even when some of the variables are slightly correlated\n",
    "* One of the traditional statistical linear models - for the purpose indentifying influencing feature \n",
    "* We focus on ML framework, meaning we need a \"prediction\" part as well as story telling\n",
    "\n",
    "![title](image6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Definition of Logistic Regression\n",
    "* It assumes a parametric form for the distribution $P(y|X)$, then  directly  estimates  its  parameters  from  the  training  data\n",
    "* The parametric model assumed by Logistic Regression in the case where $y$ is boolean (binary representation) is:\n",
    "\n",
    "\\begin{align}\n",
    "\\ P(y=1|X) = \\frac{1}{1+exp(\\beta_0 + \\sum_{i=1}^n \\beta_i X_i)} \\\\\n",
    "\\\\\n",
    "P(y=0|X) = \\frac{exp(\\beta_0 + \\sum_{i=1}^n \\beta_i X_i)}{1+exp(\\beta_0 + \\sum_{i=1}^n \\beta_i X_i)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "* Based on the sigmoid (logit) function\n",
    "![title](image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Mathematical Expression\n",
    "* Logistic Regression can be expressed as:\n",
    "\\begin{align}\n",
    "Logit(y) = \\log(\\frac{P(y=1|X)}{1-P(y=1|X)}) = \\log(\\frac{P(y=1|X)}{P(y=0|X)}) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "P(y=1|X) = \\frac{exp(\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n)}{1+exp(\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n)}\n",
    "\\end{align}\n",
    "\n",
    "* We call the term in the $log()$ function “odds” - probability of event divided by probability of no event (which describes the ratio of success to ratio of failure)\n",
    "* As converting $y$ to $Logit(y)$ or log odds($y$), the folmula is now similar to the OLS\n",
    "* Find the best fitted line to predict $Logit(y)$\n",
    "* Instead of the slope coefficients ($\\beta_i$) being the rate of change in $y$ (the dependent variables) as $x_i$ changes like OLS model, now the slope coefficient is interpreted as the rate of change in the \"log odds\" as $x_i$ changes.\n",
    "* Increasing $x_i$ by 1 increases the log-odds by $\\beta_i$. So if $x_i$ increases by 1, the odds that $y$ = 1 increase by a factor of $exp^{\\beta_{i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: if you have a logistic regression model to predict vip customer group (1 or 0) of customers based on their age\n",
    "    \n",
    "\\begin{align}\n",
    "Logit(p) = log(\\frac{p}{1-p}) = \\beta_0 + \\beta_{age} \\cdot x_{age}    \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "log(\\frac{p}{1-p})(x_{age} = 30) = \\beta_0 + \\beta_{age} \\cdot 30    \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "log(\\frac{p}{1-p})(x_{age} = 31) = \\beta_0 + \\beta_{age} \\cdot 31    \n",
    "\\end{align}\n",
    "\n",
    "Taking the difference of the two equations, we have the following:\n",
    "\\begin{align}\n",
    "log(\\frac{p}{1-p})(x_{age} = 31) - log(\\frac{p}{1-p})(x_{age} = 30) = \\beta_{age}   \n",
    "\\end{align}\n",
    "\n",
    "We can say now that the coefficient ($\\beta_{age}$) for age is the difference in the log odds.  In other words, for a one-unit increase in the customer age, the expected change in log odds is $\\beta_{age}$.\n",
    "\n",
    "If we exponentiate both sides of our last equation, we have the following:\n",
    "\\begin{align}\n",
    "exp^{log(\\frac{p}{1-p})(x_{age} = 31) - log(\\frac{p}{1-p})(x_{age} = 30)} = \\frac{exp^{log(\\frac{p}{1-p})(x_{age} = 31)}}{exp^{log(\\frac{p}{1-p})(x_{age} = 30)}} = \\frac{odds(x_{age} = 31)}{odds(x_{age} = 30)} =  exp^{\\beta_{age}}   \n",
    "\\end{align}\n",
    "\n",
    "#### Interpretation\n",
    "So we can say for a one-unit increase in age, we expect to see about $exp^{\\beta_{age}}$ times increase in the odds of being in an customer class 1 (vip group). For example, if $exp^{\\beta_{age}}$ is 1.17, then we can say one-unit increase in age, it is associated with 17% more of being in the vip customer group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) What are the Pros and Cons of Logistic Regression?\n",
    "#### Pros\n",
    "* Efficient! It does not require too many computational resources\n",
    "* Less bias\n",
    "* It works reasonably well even when some of the variables are slightly correlated (compared to Naive Bayes)\n",
    "* Interpretable coefficients are provided\n",
    "\n",
    "#### Cons\n",
    "* It can’t solve non-linear problems with logistic regression since it’s decision surface is linear\n",
    "* It requires relatively large data (compared to Naive Bayes)\n",
    "* It does work better when you remove attributes that are unrelated to the target variable\n",
    "* Feature Engineering is requred for the model performance\n",
    "* Imbalanced data*** (What if you have 9990 class 1 and only 10 class 0?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Model Evaluation Metrics for Binary Classification\n",
    "\n",
    "### Confusion matrix\n",
    "![title](image7.png)\n",
    "\n",
    "__Accuracy__<br>\n",
    "Fraction of correctly classified samples\n",
    "\n",
    "__Precision__<br>\n",
    "Fraction of relevant instances among the retrieved instances\n",
    "\n",
    "__Sensitivity (Recall)__<br>\n",
    "Fraction of the total amount of relevant instances that were actually retrieved\n",
    "\n",
    "__Specificity__<br>\n",
    "Proportion of actual negatives that are correctly identified as such"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Keep the end goal in mind!__<br>\n",
    "We are usually intersted not just in making accurate predictions, but in using these predictions as part of larger decision making process.\n",
    "\n",
    "__Handle imbalanced data__<br>\n",
    "What if we have a dataset with 100 samples are class 1 and 9900 samples are class 0 and have a confusion matrix like this:\n",
    "![title](image8.png)\n",
    "\n",
    "__Over-sampling or under-sampling methods__<br>\n",
    "* Random Oversampling: Randomly duplicate examples in the minority class\n",
    "* Random Undersampling: Randomly delete examples in the majority class\n",
    "* SMOTE (Synthetic Minority Oversampling Technique): It uses a nearest neighbors algorithm to generate new and synthetic data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve (Receiver Operating Characteristic) and AUC (Area Under The Curve)\n",
    "* The most important evaluation metrics for checking any classification model’s performance\n",
    "* A performance measurement for classification problem at various thresholds settings\n",
    "* The ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis\n",
    "* TPR (True positive rate) = Sensitivity or Recall = TP / (TP+FN)\n",
    "* FPR (False positive rate) = 1 - Specificity = FP / (TN+FP)\n",
    "\n",
    "__Ideal case__<br>\n",
    "When two curves don’t overlap at all means model has an ideal measure of separability. It is perfectly able to distinguish between positive class and negative class.\n",
    "![title](image9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reasonable case__<br>\n",
    "When two distributions overlap, we introduce type 1 and type 2 error. Depending upon the threshold, we can minimize or maximize them. When AUC is 0.7, it means there is 70% chance that model will be able to distinguish between positive class and negative class.\n",
    "![title](image10.png)\n",
    "\n",
    "__The worst case__ <br>\n",
    "The model has no discrimination capacity to distinguish between positive class and negative class.\n",
    "![title](image11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python example: Predicting neighborhoods' 311 reporting behavior by using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will use the dataset about 311 reporting behavior and socioeconomic characteristics. This dataset is aggregated at the Census Block Group (CBG) level and the followings are the description of the columns.\n",
    "\n",
    "* target (y)    \n",
    "    * Target class 1: Neighborhoods over-reporting local problems\n",
    "    * Target class 0: Neighborhoods under-reporting local problems\n",
    "* female_r: % of female population\n",
    "* elderly_r: % of population older than 70\n",
    "* black_r: % of black population\n",
    "* non_eng_spanish_r: % of limited English speaker using Spanish\n",
    "* non_eng_asian_r: % of limited English speaker using Asian languages\n",
    "* hh_kid_r: % of housholds with kids under 18\n",
    "* edu_high_r: % of population with at least Bachelor's degree\n",
    "* unemployed_r: Unemployment rate\n",
    "* med_rent: Median rent (USD)\n",
    "* longercommute_r: % of commuters taking more than 45 minutes\n",
    "* nonprofit_r: #of nonprofit organizaiton per 1000 people\n",
    "* meanvt_rat: voter turn out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the sample data and split data randomly into the training (75%) and test (25%) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use 7 predictors at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['female_r', 'elderly_r', 'black_r', 'non_eng_spanish_r', 'non_eng_asian_r', 'hh_kid_r', 'edu_high_r', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.iloc[:,-1]\n",
    "X = data.iloc[:,:-1]\n",
    "\n",
    "# Split train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.75, random_state=10)\n",
    "\n",
    "# Check the number of samples\n",
    "print (\"Train sample (X):\", len(X_train))\n",
    "print (\"Train sample (y):\", len(y_train))\n",
    "print (\"Test sample (X):\", len(X_test))\n",
    "print (\"Test sample (y):\", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Logistic Regression\n",
    "\n",
    "    YOUR_MODEL_NAME = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy of training model\n",
    "\n",
    "    YOUR_MODEL_NAEM.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy:\",round(model.score(X_train, y_train),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression coefficietns and odds ratio\n",
    "\n",
    "    Regression coefficients (log odds) = YOUR_MODEL_NAME.coef_\n",
    "    Odds ratio = np.exp(Regression coefficients) = np.exp(YOUR_MODEL_NAME.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_results = pd.DataFrame()\n",
    "para_results['feature'] = data.columns.tolist()[:-1]\n",
    "para_results['odds_ratio'] = odds_ratio[0]\n",
    "para_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking accuracy is not enough to evaluate your model performance, especially for the binary classification case!\n",
    "#### Let's check a confusion matrix\n",
    "\n",
    "    Confusion matrix = confusion_matrix(y_trian, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the confusion matrix, we can calculate 1) accuracy, 2) recall, 3) precision, and 4) specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "# [[TN, FP]\n",
    "#  [FN, TP]]\n",
    "\n",
    "TN = confusion[0][0]\n",
    "FP = confusion[0][1]\n",
    "FN = confusion[1][0]\n",
    "TP = confusion[1][1]\n",
    "\n",
    "Accuracy = (TN + TP) / (TN + FP + FN + TP)\n",
    "Recall = (TP) / (TP + FN)\n",
    "Precision = (TP) / (TP+FP)\n",
    "Specificity = (TN) / (TN+FP)\n",
    "\n",
    "print (\"Accuracy:\",Accuracy)\n",
    "print (\"Recall:\",Recall)\n",
    "print (\"Precision:\",Precision)\n",
    "print (\"Specificity:\",Specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another model evaluation metric - ROC curve and AUC\n",
    "\n",
    "    Firstly, you need to compute probability that a data point is assigned as either class 1 or class 2\n",
    "    probability = YOUR_MODEL_NAME.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate FPR (False Positive Rate) and TPR (True Positive Rate) to create a ROC curve\n",
    "    metrics.roc_curve() function provides FPR, TPR, and threshold\n",
    "    metrics.acut() provides AUC (area under curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = y_prob[:,1]  # we usually take the probability of class 1\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_train, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# Let's create ROC curve with AUC score\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Let's apply to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "# [[TN, FP]\n",
    "#  [FN, TP]]\n",
    "confusion = confusion_matrix(y_test,y_pred_test)\n",
    "\n",
    "print (confusion)\n",
    "\n",
    "TN = confusion[0][0]\n",
    "FP = confusion[0][1]\n",
    "FN = confusion[1][0]\n",
    "TP = confusion[1][1]\n",
    "\n",
    "Accuracy = (TN + TP) / (TN + FP + FN + TP)\n",
    "Recall = (TP) / (TP + FN)\n",
    "Precision = (TP) / (TP+FP)\n",
    "Specificity = (TN) / (TN+FP)\n",
    "\n",
    "print (\"Accuracy:\",round(Accuracy,3))\n",
    "print (\"Recall:\",round(Recall,3))\n",
    "print (\"Precision:\",round(Precision,3))\n",
    "print (\"Specificity:\",round(Specificity,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: <br>\n",
    "__Although the prediction accuracy is not perfect, we can reasonably predict whether a neighborhood over-report of under-report about their local problems through 311 system, using logistic regression based on neighborhood demogarphics and socioeconomics!__\n",
    "\n",
    "__Please think about how to use ML framework for urban planning context.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Urban Example\n",
    "## Applications of machine learning methods to predict readmission and length-of-stay for homeless families: the case of WIN shelters in New York City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
